{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # LangChain Core Modules: Model I/O\n",
    "\n",
    "`Model I/O` is a standardized interface provided by LangChain for working with large language models (LLMs). It includes components for model inputs (Prompts), outputs (Output Parsers), and the models themselves (Models).\n",
    "\n",
    "- Prompts: Template-based, dynamic selection and managed model inputs. 模板化、动态选择和管理模型输入\n",
    "\n",
    "- Models: Standard interface for calling language models. 以通用接口调用语言模型\n",
    "\n",
    "- Output Parsers: Extract and normalize information from model outputs. 从模型输出中提取信息，并规范化内容\n",
    "\n",
    "![](../../images/model_io.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Python SDK（https://github.com/langchain-ai/langchain）\n",
    "# !pip install -U langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Abstraction 模型抽象\n",
    "\n",
    "- Language Models (LLMs): The core component of LangChain. LangChain does not provide its own LLMs; instead, it offers a standardized interface to interact with a variety of LLMs (such as OpenAI, Cohere, Hugging Face, etc.). LangChain 的核心组件。LangChain并不提供自己的LLMs，而是为与许多不同的LLMs（OpenAI、Cohere、Hugging Face等）进行交互提供了一个标准接口。\n",
    "\n",
    "- Chat Models（聊天模型）: A variant of language models. Although chat models internally use language models, their interfaces are slightly different. Rather than exposing a \"text in, text out\" API, they use an interface based on \"chat messages\" as both input and output. 语言模型的一种变体。虽然聊天模型在内部使用了语言模型，但它们提供的接口略有不同。与其暴露一个“输入文本，输出文本”的API不同，它们提供了一个以“聊天消息”作为输入和输出的接口。\n",
    "\n",
    "(Note: Compare OpenAI's Completion API and Chat Completion API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型（LLMs)\n",
    "\n",
    "Class Inheritance Hierarchy：\n",
    "\n",
    "LangChain organizes its model abstractions using an inheritance-based structure:\n",
    "\n",
    "```\n",
    "BaseLanguageModel\n",
    "└── BaseLLM (for text-based language models)\n",
    "    └── Specific implementations (e.g., OpenAI, Cohere, AI21, HuggingFaceHub, etc.)\n",
    "\n",
    "BaseChatModel\n",
    "└── ChatOpenAI, ChatAnthropic, etc.\n",
    "```\n",
    "\n",
    "This structure allows both LLMs and Chat Models to share common behavior while supporting interface differences.\n",
    "\n",
    "\n",
    "Main Abstract:\n",
    "\n",
    "```\n",
    "LLMResult, PromptValue,\n",
    "CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\n",
    "CallbackManager, AsyncCallbackManager,\n",
    "AIMessage, BaseMessage\n",
    "```\n",
    "\n",
    "**API Reference：https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.llms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLanguageModel Class\n",
    "\n",
    "**https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/language_model.py**\n",
    "\n",
    "这个基类为语言模型定义了一个接口，该接口允许用户以不同的方式与模型交互（例如通过提示或消息）。`generate_prompt` 是其中的一个主要方法，它接受一系列提示，并返回模型的生成结果。\n",
    "\n",
    "```python\n",
    "# 定义 BaseLanguageModel 抽象基类，它从 Serializable, Runnable 和 ABC 继承\n",
    "class BaseLanguageModel(\n",
    "    Serializable, Runnable[LanguageModelInput, LanguageModelOutput], ABC\n",
    "):\n",
    "    \"\"\"\n",
    "    与语言模型交互的抽象基类。\n",
    "\n",
    "    所有语言模型的封装器都应从 BaseLanguageModel 继承。\n",
    "\n",
    "    主要提供三种方法：\n",
    "    - generate_prompt: 为一系列的提示值生成语言模型输出。提示值是可以转换为任何语言模型输入格式的模型输入（如字符串或消息）。\n",
    "    - predict: 将单个字符串传递给语言模型并返回字符串预测。\n",
    "    - predict_messages: 将一系列 BaseMessages（对应于单个模型调用）传递给语言模型，并返回 BaseMessage 预测。\n",
    "\n",
    "    每种方法都有对应的异步方法。\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义一个抽象方法 generate_prompt，需要子类进行实现\n",
    "    @abstractmethod\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        prompts: List[PromptValue],  # 输入提示的列表\n",
    "        stop: Optional[List[str]] = None,  # 生成时的停止词列表\n",
    "        callbacks: Callbacks = None,  # 回调，用于执行例如日志记录或流式处理的额外功能\n",
    "        **kwargs: Any,  # 任意的额外关键字参数，通常会传递给模型提供者的 API 调用\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"\n",
    "        将一系列的提示传递给模型并返回模型的生成。\n",
    "\n",
    "        对于提供批处理 API 的模型，此方法应使用批处理调用。\n",
    "\n",
    "        使用此方法时：\n",
    "            1. 希望利用批处理调用，\n",
    "            2. 需要从模型中获取的输出不仅仅是最顶部生成的值，\n",
    "            3. 构建与底层语言模型类型无关的链（例如，纯文本完成模型与聊天模型）。\n",
    "\n",
    "        参数:\n",
    "            prompts: 提示值的列表。提示值是一个可以转换为与任何语言模型匹配的格式的对象（对于纯文本生成模型为字符串，对于聊天模型为 BaseMessages）。\n",
    "            stop: 生成时使用的停止词。模型输出在这些子字符串的首次出现处截断。\n",
    "            callbacks: 要传递的回调。用于执行额外功能，例如在生成过程中进行日志记录或流式处理。\n",
    "            **kwargs: 任意的额外关键字参数。通常这些会传递给模型提供者的 API 调用。\n",
    "\n",
    "        返回值:\n",
    "            LLMResult，它包含每个输入提示的候选生成列表以及特定于模型提供者的额外输出。\n",
    "        \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLLM Class\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/base.py**\n",
    "\n",
    "这段代码定义了一个名为 BaseLLM 的抽象基类。这个基类的主要目的是提供一个基本的接口来处理大型语言模型 (LLM)。\n",
    "\n",
    "```python\n",
    "# 定义 BaseLLM 抽象基类，它从 BaseLanguageModel[str] 和 ABC（Abstract Base Class）继承\n",
    "class BaseLLM(BaseLanguageModel[str], ABC):\n",
    "    \"\"\"Base LLM abstract interface.\n",
    "    \n",
    "    It should take in a prompt and return a string.\"\"\"\n",
    "\n",
    "    # 定义可选的缓存属性，其初始值为 None\n",
    "    cache: Optional[bool] = None\n",
    "\n",
    "    # 定义 verbose 属性，该属性决定是否打印响应文本\n",
    "    # 默认值使用 _get_verbosity 函数的结果\n",
    "    verbose: bool = Field(default_factory=_get_verbosity)\n",
    "    \"\"\"Whether to print out response text.\"\"\"\n",
    "\n",
    "    # 定义 callbacks 属性，其初始值为 None，并从序列化中排除\n",
    "    callbacks: Callbacks = Field(default=None, exclude=True)\n",
    "\n",
    "    # 定义 callback_manager 属性，其初始值为 None，并从序列化中排除\n",
    "    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\n",
    "\n",
    "    # 定义 tags 属性，这些标签会被添加到运行追踪中，其初始值为 None，并从序列化中排除\n",
    "    tags: Optional[List[str]] = Field(default=None, exclude=True)\n",
    "    \"\"\"Tags to add to the run trace.\"\"\"\n",
    "\n",
    "    # 定义 metadata 属性，这些元数据会被添加到运行追踪中，其初始值为 None，并从序列化中排除\n",
    "    metadata: Optional[Dict[str, Any]] = Field(default=None, exclude=True)\n",
    "    \"\"\"Metadata to add to the run trace.\"\"\"\n",
    "\n",
    "    # 内部类定义了这个 pydantic 对象的配置\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        # 允许使用任意类型\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "```\n",
    "这个基类使用了 Pydantic 的功能，特别是 Field 方法，用于定义默认值和序列化行为。BaseLLM 的子类需要提供实现具体功能的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Class\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/base.py**\n",
    "\n",
    "这段代码定义了一个名为 LLM 的类，该类继承自 BaseLLM。这个类的目的是为了为用户提供一个简化的接口来处理LLM（大型语言模型），而不期望用户实现完整的 _generate 方法。\n",
    "\n",
    "```python\n",
    "\n",
    "# 继承自 BaseLLM 的 LLM 类\n",
    "class LLM(BaseLLM):\n",
    "    \"\"\"Base LLM abstract class.\n",
    "\n",
    "    The purpose of this class is to expose a simpler interface for working\n",
    "    with LLMs, rather than expect the user to implement the full _generate method.\n",
    "    \"\"\"\n",
    "\n",
    "    # 使用 @abstractmethod 装饰器定义一个抽象方法，子类需要实现这个方法\n",
    "    @abstractmethod\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,  # 输入提示\n",
    "        stop: Optional[List[str]] = None,  # 停止词列表\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,  # 运行管理器\n",
    "        **kwargs: Any,  # 其他关键字参数\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n",
    "        # 此方法的实现应在子类中提供\n",
    "\n",
    "    # _generate 方法使用了 _call 方法，用于处理多个提示\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],  # 多个输入提示的列表\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n",
    "        # TODO: 在此处添加缓存逻辑\n",
    "        generations = []  # 用于存储生成的文本\n",
    "        # 检查 _call 方法的签名是否支持 run_manager 参数\n",
    "        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n",
    "        for prompt in prompts:  # 遍历每个提示\n",
    "            # 根据是否支持 run_manager 参数来选择调用方法\n",
    "            text = (\n",
    "                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "                if new_arg_supported\n",
    "                else self._call(prompt, stop=stop, **kwargs)\n",
    "            )\n",
    "            # 将生成的文本添加到 generations 列表中\n",
    "            generations.append([Generation(text=text)])\n",
    "        # 返回 LLMResult 对象，其中包含 generations 列表\n",
    "        return LLMResult(generations=generations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs 已支持模型清单\n",
    "\n",
    "**开发者文档：https://python.langchain.com/docs/integrations/llms/**\n",
    "** https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.BaseOpenAI.html#langchain_openai.llms.base.BaseOpenAI **\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/llms**\n",
    "\n",
    "### 使用 LangChain 调用 OpenAI GPT Completion API\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/openai.py**\n",
    "\n",
    "#### BaseOpenAI Class\n",
    "\n",
    "```python\n",
    "class BaseOpenAI(BaseLLM):\n",
    "    \"\"\"OpenAI 大语言模型的基类。\"\"\"\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
    "\n",
    "    @property\n",
    "    def lc_serializable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    client: Any  #: :meta private:\n",
    "    model_name: str = Field(\"text-davinci-003\", alias=\"model\") #使用的模型名\n",
    "    temperature: float = 0.7 #要使用的采样温度。\n",
    "    max_tokens: int = 256 #完成中生成的最大token数。 -1表示根据提示和模型的最大上下文大小返回尽可能多的token。\n",
    "    top_p: float = 1 # Total probability mass of tokens to consider at each step.\n",
    "    frequency_penalty: float = 0 # Penalizes repeated tokens according to frequency.\n",
    "    presence_penalty: float = 0 # Penalizes repeated tokens.\n",
    "    n: int = 1 # How many completions to generate for each prompt\n",
    "    best_of: int = 1 # Generates best_of completions server-side and returns the “best”.\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    openai_api_key: Optional[str] = None\n",
    "    openai_api_base: Optional[str] = None\n",
    "    openai_organization: Optional[str] = None\n",
    "    # 支持OpenAI的显式代理\n",
    "    openai_proxy: Optional[str] = None\n",
    "    batch_size: int = 20\n",
    "    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "    logit_bias: Optional[Dict[str, float]] = Field(default_factory=dict)\n",
    "    max_retries: int = 6\n",
    "    streaming: bool = False\n",
    "    allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set()\n",
    "    disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\"\n",
    "    tiktoken_model_name: Optional[str] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\" The assistant responds, \"Why did the scarecrow win an award? Because he was outstanding in his field!\" \n",
      "\n",
      "This interaction captures the essence of the user asking for humor and the assistant providing it in a light-hearted manner. If you have any specific requests or topics you'd like a joke about, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Tell me a Joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比直接调用Open AI 的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "Why did the scarecrow win an award? \n",
      "Because he was outstanding in\n",
      "CompletionUsage(completion_tokens=16, prompt_tokens=4, total_tokens=20, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "{\n",
      "  \"id\": \"cmpl-BejNXkQal4ITCPK79LORNtvwdF5hM\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"!\\nWhy did the scarecrow win an award? \\nBecause he was outstanding in\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1749047255,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"completion\",\n",
      "  \"system_fingerprint\": \"fp_34a54ae93c\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"prompt_tokens\": 4,\n",
      "    \"total_tokens\": 20,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.completions.create(model='gpt-4o-mini', prompt=\"Tell me a Joke\")\n",
    "\n",
    "print(completion.choices[0].text)\n",
    "print(dict(completion).get('usage'))\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.max_tokens = 1024\n",
    "llm.max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "    return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Example usage\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quick_sort(arr)\n",
      "print(sorted_arr)  # Output: [1, 1, 2, 3, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"generate a quick sort algorithm in python\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaingChain 的LLM 抽象维护了openAI 的连接状态（参数设定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Tell me a Joke\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天模型（Chat Models)\n",
    "\n",
    "Class Inheritance Hierarchy：\n",
    "\n",
    "```\n",
    "BaseChatModel\n",
    "└── ChatOpenAI, ChatAnthropic, etc.\n",
    "```\n",
    "\n",
    "Main Abstract:\n",
    "\n",
    "```\n",
    "AIMessage, BaseMessage, HumanMessage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseChatModel Class\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chat_models/base.py**\n",
    "\n",
    "\n",
    "```python\n",
    "class BaseChatModel(BaseLanguageModel[BaseMessageChunk], ABC):\n",
    "    cache: Optional[bool] = None\n",
    "    \"\"\"是否缓存响应。\"\"\"\n",
    "    verbose: bool = Field(default_factory=_get_verbosity)\n",
    "    \"\"\"是否打印响应文本。\"\"\"\n",
    "    callbacks: Callbacks = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的回调函数。\"\"\"\n",
    "    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的回调函数管理器。\"\"\"\n",
    "    tags: Optional[List[str]] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的标签。\"\"\"\n",
    "    metadata: Optional[Dict[str, Any]] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的元数据。\"\"\"\n",
    "\n",
    "    # 需要子类实现的 _generate 抽象方法\n",
    "    @abstractmethod\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatOpenAI Class（调用 Chat Completion API）\n",
    "\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chat_models/openai.py**\n",
    "\n",
    "```python\n",
    "class ChatOpenAI(BaseChatModel):\n",
    "    \"\"\"OpenAI Chat大语言模型的包装器。\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
    "\n",
    "    @property\n",
    "    def lc_serializable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    client: Any = None  #: :meta private:\n",
    "    model_name: str = Field(default=\"gpt-3.5-turbo\", alias=\"model\")\n",
    "    temperature: float = 0.7\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    openai_api_key: Optional[str] = None\n",
    "    \"\"\"API请求的基础URL路径，\n",
    "    如果不使用代理或服务仿真器，请留空。\"\"\"\n",
    "    openai_api_base: Optional[str] = None\n",
    "    openai_organization: Optional[str] = None\n",
    "    # 支持OpenAI的显式代理\n",
    "    openai_proxy: Optional[str] = None\n",
    "    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "    \"\"\"请求OpenAI完成API的超时。默认为600秒。\"\"\"\n",
    "    max_retries: int = 6\n",
    "    streaming: bool = False\n",
    "    n: int = 1\n",
    "    max_tokens: Optional[int] = None\n",
    "    tiktoken_model_name: Optional[str] = None\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-mini', \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BfIZgYV7ntRJRgbZpeWSSfQjeg7Qk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was notable because it was a neutral site due to the COVID-19 pandemic, and it was the first time a World Series was played at a neutral location since the World Series started in 1903.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1749182548, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=59, prompt_tokens=53, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was notable because it was a neutral site due to the COVID-19 pandemic, and it was the first time a World Series was played at a neutral location since the World Series started in 1903.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))]\n",
      "CompletionUsage(completion_tokens=59, prompt_tokens=53, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "{\n",
      "  \"id\": \"chatcmpl-BfIZgYV7ntRJRgbZpeWSSfQjeg7Qk\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was notable because it was a neutral site due to the COVID-19 pandemic, and it was the first time a World Series was played at a neutral location since the World Series started in 1903.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null,\n",
      "        \"annotations\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1749182548,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_34a54ae93c\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 59,\n",
      "    \"prompt_tokens\": 53,\n",
      "    \"total_tokens\": 112,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices)\n",
    "print(dict(completion).get('usage'))\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [SystemMessage(content=\"You are a helpful assistant.\"),\n",
    " HumanMessage(content=\"Who won the world series in 2020?\"),\n",
    " AIMessage(content=\"The Los Angeles Dodgers won the World Series in 2020.\"), \n",
    " HumanMessage(content=\"Where was it played?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who won the world series in 2020?', additional_kwargs={}, response_metadata={}), AIMessage(content='The Los Angeles Dodgers won the World Series in 2020.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where was it played?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was notable because it was the first time the World Series was played at a neutral site due to the COVID-19 pandemic.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 53, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run--ca2141fb-f034-4f9e-a690-3ea711dab7fc-0', usage_metadata={'input_tokens': 53, 'output_tokens': 43, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was a deviation from the traditional site of alternating games between the two teams' home stadiums, as it was held at a neutral site due to the COVID-19 pandemic.\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"refusal\": null\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 53,\n",
      "      \"prompt_tokens\": 53,\n",
      "      \"total_tokens\": 106,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_62a23a81ef\",\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"logprobs\": null\n",
      "  },\n",
      "  \"type\": \"ai\",\n",
      "  \"name\": null,\n",
      "  \"id\": \"run--b3870ece-7848-4de4-9f7a-845c39334759-0\",\n",
      "  \"example\": false,\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 53,\n",
      "    \"output_tokens\": 53,\n",
      "    \"total_tokens\": 106,\n",
      "    \"input_token_details\": {\n",
      "      \"audio\": 0,\n",
      "      \"cache_read\": 0\n",
      "    },\n",
      "    \"output_token_details\": {\n",
      "      \"audio\": 0,\n",
      "      \"reasoning\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(chat_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agents101 Env",
   "language": "python",
   "name": "agents101_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
