{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b42dcc-c644-45a6-8a6a-3b250c74cee7",
   "metadata": {},
   "source": [
    "## Langchain Expression Language（LCEL）Quick Start\n",
    "\n",
    "LCEL is a declarative way to compose chains and agents in LangChain, introduced to simplify building and managing chains of components like LLMs, tools, retrievers, etc.\n",
    "Think of it as a functional programming layer over LangChain where you build pipelines using a clean, composable syntax (like Lego blocks).\n",
    "\n",
    "### Beneifts\n",
    "\n",
    "1. Composable Pipelines\n",
    "You can use | to chain components, making it intuitive and readable.\n",
    "Supports branching, merging, mapping, etc.\n",
    "\n",
    "2. Type Safety & Debuggability\n",
    "More predictable inputs/outputs between steps.\n",
    "Easier to log, debug, and trace flows.\n",
    "\n",
    "3. Better for Production\n",
    "All chains are Runnable, so they support:\n",
    "    Streaming\n",
    "    Tracing\n",
    "    Batching\n",
    "    Concurrency\n",
    "\n",
    "4. Unified Design\n",
    "Works across agents, chains, retrievers, prompts, tools, etc.\n",
    "Lets you build complex logic without subclassing or imperative glue code\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1. Learning Curve\n",
    "It's newer and less documented than traditional LangChain chains.\n",
    "May be harder for beginners without functional programming background.\n",
    "\n",
    "2. Less Flexibility in Some Cases\n",
    "For highly dynamic or stateful workflows (e.g. agent memory loops), LCEL may feel restrictive.\n",
    "\n",
    "3. Ecosystem Still Evolving\n",
    "Some wrappers, tools, or libraries might not support LCEL yet.\n",
    "Not all legacy chains or components are easily portable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b55489-48a5-4c2e-9a6e-3f246f0881c6",
   "metadata": {},
   "source": [
    "### Using LCEL to Build an LLMChain（Prompt + LLM)\n",
    "\n",
    "#### Pipe Operator\n",
    "\n",
    "Use LCEL's `|` operator to connect these different components into a single chain.\n",
    "\n",
    "**In this chain, the user's input is passed to the prompt template, then the output of the prompt is passed to the model, and finally the model's output is passed to the output parser.**\n",
    "\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624f2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108de668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2bcf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdb5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09242ff5-b5ff-4902-9de8-59e4af945bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do software engineers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "\n",
    "# Initialize the output parser to convert the model's output into a string\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Build a processing chain that first generates a complete input using a prompt template, then processes it with the model, and finally parses the output.\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# Call the processing chain and pass in the topic \"x\" to generate a joke about x.\n",
    "chain.invoke({\"topic\": \"software engineers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953507b",
   "metadata": {},
   "source": [
    "### Step by Step Breakdown without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af28cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Prompt  Not using LCEL\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell a joke about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4b0ead-76d5-43e1-ba40-590b20894c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell a joke about software engineers')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt_value = prompt_template.invoke({\"topic\": \"software engineers\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea46f7e-67cb-44c5-a5c1-6311a2a75639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell a joke about software engineers', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6030b5-f19f-42ee-9405-0bc229bd3c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell a joke about software engineers'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82e786d-52f1-4082-92b5-00f3029d5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini model is a ChatModel, and its invoke method will return a BaseMessage.\n",
    "message = model.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6a7a85-c1da-4441-a1db-cab6d0bd32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why do software engineers prefer dark mode?\\n\\nBecause light attracts bugs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 13, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-fdc06e0c-f8ce-461c-94d5-78b16fe16685-0', usage_metadata={'input_tokens': 13, 'output_tokens': 14, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150ceaf0-fcc1-4756-8fca-ab9bf28ed94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do software engineers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The message is processed by StrOutputParser and converted into a standard string.\n",
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b56176b-65d6-41f4-8071-058593e47a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy was the software engineer afraid of the debugger?\\n\\nBecause it kept finding all his bugs!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using an LLM model like gpt-3.5-turbo-instruct, the invoke method will return a string.\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e1741-83a4-4312-be93-76523e2c9373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02797bf-df69-4c7d-ba0a-35df38d53173",
   "metadata": {},
   "source": [
    "## Invoke In Retrieval (RAG)\n",
    "\n",
    "\n",
    "```python\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "```\n",
    "\n",
    "First, set up a tretriever using a memory-based store that can fetch documents based on a query. This retriever is also a runnable component and can be linked with other components, can also running it along.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. First create a `RunnableParallel` object **setup_and_retrieval** with two entries。the first entry `context` will include the document results retrieved by the retriever. The second entry `question` will contain the original user question. To pass through the question, we use `RunnablePassthrough` to copy that entry,\n",
    "2. The dictionary from the previous step is passed to the `Prompt` component. It receives user input along with the retrieved documents (i.e., the context), construct the prompt, and outputs a PromptValue.\n",
    "3. `Model` component receives the generated prompt and passes it to the Opean AI `gpt-4o-mini` model for evaluation。The output generated by the model is a ChatMessage object.\n",
    "4. Finally, `output_parser` component receives the ChatMessage and converts it into a Python string, which is then retruned by the invoke method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa1a415-c38e-44c3-bda1-7e50aa6f7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188130e3-54d8-4efe-a40e-c0ab81f9e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create an in-memory vector store using DocArrayInMemorySearch\n",
    "# Use OpenAIEmbeddings to generate embedding vectors for the texts\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# Convert the vector store into a retriever.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Set up a parallel runner to handle context retrieval and question passing simultaneously\n",
    "# Use RunnableParallel to prepare the expected input by combining the retrieved document entries and the original user question\n",
    "# Perform document search using the retriever, and use RunnablePassthrough to pass through the user's question.\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where does harrison work？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcecfd-e726-4570-bea6-19cd5b8fbd15",
   "metadata": {},
   "source": [
    "#### Ignore UserWarning：\n",
    "\n",
    "UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
    "\n",
    "Cause：`The issue is with pydantic version, it's 2.0.0+ and not compatible with docarray.\n",
    "Instead it should be pydantic==1.10.9`\n",
    "\n",
    "Reference：https://github.com/langchain-ai/langchain/issues/15394\n",
    "\n",
    "LangChain's document on Pydantic compatibility：https://python.langchain.com/v0.1/docs/guides/development/pydantic_compatibility/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
