{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "439df52a-dadf-4fd8-a0ad-e59bac7ca529",
   "metadata": {},
   "source": [
    "# LCEL with RAG\n",
    "\n",
    "RAG stands for Retrieval-Augmented Generation ‚Äî it's a powerful technique that combines a language model (like GPT) with an external knowledge source (like a document database) to produce more accurate and context-aware outputs.\n",
    "\n",
    "## üîÅ RAG Workflow Breakdown\n",
    "\n",
    "The entire process consists of the following key steps:\n",
    "\n",
    "1. **Load Documents**  \n",
    "   Load raw data into LangChain. The source could be online websites, local files, or various platforms.\n",
    "\n",
    "2. **Split Documents**  \n",
    "   Break the loaded documents into smaller chunks. This helps fit within the model's context window and makes vector embedding and retrieval more effective.\n",
    "\n",
    "3. **Store Embeddings**  \n",
    "   Convert the document chunks into vector embeddings and store them in a vector database for future retrieval.\n",
    "\n",
    "4. **Retrieve Documents**  \n",
    "   Query the vector database to retrieve the most relevant document chunks based on the user‚Äôs question.\n",
    "\n",
    "5. **Generate Answer**  \n",
    "   Combine the retrieved context with the user query and feed it into the LLM to generate a final answer.\n",
    "\n",
    "\n",
    "\n",
    "By following these steps, you can build a powerful Q&A system that decomposes complex tasks into smaller steps and generates detailed, accurate responses.\n",
    "\n",
    "![rag](../../images/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d81c83-4083-4ec1-ac4c-e96840024278",
   "metadata": {},
   "source": [
    "### ‚úÖ Benefits of Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "> RAG enhances LLM capabilities by combining external knowledge retrieval with generation.\n",
    "\n",
    "#### üîπ Reduces Hallucination  \n",
    "Retrieves grounded knowledge to reduce fabricated or inaccurate responses.\n",
    "\n",
    "#### üîπ Updatable Knowledge  \n",
    "No need to retrain models ‚Äî just update the data source or vector store.\n",
    "\n",
    "#### üîπ Domain-Specific Intelligence  \n",
    "Supports use of private, internal, or specialized content (e.g., legal, medical, technical docs).\n",
    "\n",
    "#### üîπ Cost Efficient  \n",
    "Cheaper and faster than fine-tuning or training large models.\n",
    "\n",
    "#### üîπ Transparent Reasoning  \n",
    "Retrieved content can be shown alongside the answer for traceability and trust.\n",
    "\n",
    "#### üîπ Modular Architecture  \n",
    "Each component (retriever, vector DB, LLM) can be swapped or tuned independently.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Limitations of Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "> While powerful, RAG introduces new design and performance challenges.\n",
    "\n",
    "#### ‚ö†Ô∏è Garbage In, Garbage Out  \n",
    "Poor retrieval or bad data will lead to misleading or unhelpful answers.\n",
    "\n",
    "#### ‚ö†Ô∏è Latency Overhead  \n",
    "Document retrieval and embedding lookup adds time to each query.\n",
    "\n",
    "#### ‚ö†Ô∏è Chunking & Embedding Quality  \n",
    "Bad chunking or embedding can drastically lower relevance and retrieval accuracy.\n",
    "\n",
    "#### ‚ö†Ô∏è Limited Cross-Chunk Reasoning  \n",
    "LLMs can‚Äôt always reason well over multiple small chunks due to token window constraints.\n",
    "\n",
    "#### ‚ö†Ô∏è Manual Updates Needed  \n",
    "Keeping the vector store current requires periodic data reprocessing.\n",
    "\n",
    "#### ‚ö†Ô∏è Overkill for Simple Tasks  \n",
    "Adds complexity where a pure LLM might be faster and good enough.\n",
    "\n",
    "---\n",
    "\n",
    "> üß† RAG is ideal when your use case needs accuracy, up-to-date info, or grounded answers from private knowledge sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd162a21",
   "metadata": {},
   "source": [
    "## üß† Summary: Building a Vector Store Involves\n",
    "\n",
    "| **Stage**     | **Options**                                                                 |\n",
    "|---------------|------------------------------------------------------------------------------|\n",
    "| **Chunking**  | Sentence, paragraph, token-based, recursive splitters                        |\n",
    "| **Embedding** | OpenAI, HuggingFace, local models                                            |\n",
    "| **Store Backend** | FAISS, Qdrant, Pinecone, Chroma, Milvus, Weaviate                        |\n",
    "| **Frameworks**| LangChain, LlamaIndex, Haystack, custom                                      |\n",
    "| **Indexing**  | Flat, HNSW, IVF, filtering, hybrid search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b363d-c56e-420e-b8c1-76420b6cfa46",
   "metadata": {},
   "source": [
    "## **RAG Development Guide**\n",
    "\n",
    "**Build a Retrieval-Augmented Generation (RAG) application using the LangChain framework**\n",
    "\n",
    "1. **Load Documents**:  \n",
    "   Use the `WebBaseLoader` class to load content from a specified source and generate `Document` objects (depends on the `bs4` library).\n",
    "\n",
    "2. **Split Documents**:  \n",
    "   Use the `split_documents()` method from the `RecursiveCharacterTextSplitter` class to split long documents into smaller chunks.\n",
    "\n",
    "3. **Store Embeddings**:  \n",
    "   Use the `from_documents()` method from the `Chroma` class to embed the split document chunks into a vector space and store them in a vector database (using `OpenAIEmbeddings`).  \n",
    "   You can confirm successful storage by checking the number of stored vectors.\n",
    "\n",
    "4. **Retrieve Documents**:  \n",
    "   Use the `as_retriever()` and `invoke()` methods from the `VectorStoreRetriever` class to retrieve the most relevant document chunks based on a query.\n",
    "\n",
    "5. **Generate Answer**:  \n",
    "   Use the `invoke()` method from the `ChatOpenAI` class to combine the retrieved document chunks with the user‚Äôs question and generate an answer  \n",
    "   (utilizing `RunnablePassthrough` and `StrOutputParser`).\n",
    "\n",
    "\n",
    "We use the blog post *‚ÄúLLM Powered Autonomous Agents‚Äù* by Lilian Weng ([https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)) as the source document.  \n",
    "The final RAG application allows us to query relevant information from this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5318a1-2bb9-45bb-8206-82fcf024c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd3284-1cd1-4f53-b105-7f4f5d70faf3",
   "metadata": {},
   "source": [
    "### Step 1: Load Documents\n",
    "\n",
    "- **Description**: Use a `DocumentLoader` to load content from a specified source (e.g., a webpage) and convert it into `Document` objects.\n",
    "\n",
    "- **Key Code Abstractions**:\n",
    "  - Class: `WebBaseLoader`\n",
    "  - Method: `load()`\n",
    "  - Library: `bs4` (BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ccef98-31df-4a7d-a5d1-94785651e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use WebBaseLoader to load content from a webpage, keeping only the title, headers, and main article content.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5c2d11-eff2-43d1-8800-9d9d3a301f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43130\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94fd046-6a2a-4761-a83b-a9f82ed53b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26600ceb-0de8-4820-b438-ed8ae561886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6705c5-54ab-4cbd-9718-01563257e54d",
   "metadata": {},
   "source": [
    "### Step 2: Split Documents\n",
    "\n",
    "- **Description**: Use a text splitter to divide long loaded documents into smaller chunks for embedding and retrieval.\n",
    "\n",
    "- **Key Code Abstractions**:\n",
    "  - Class: `RecursiveCharacterTextSplitter`\n",
    "  - Method: `split_documents()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0958c3b5-4110-44b2-9e27-4a82015c0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RecursiveCharacterTextSplitter to split documents into chunks of 1000 characters with a 200-character overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Max size per chunk in characters\n",
    "    chunk_overlap=200, # Overlap between chunks to preserve context across boundaries\n",
    "    add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d73830-8e73-4293-80b0-7e631f7344d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5263f892-4a80-42b9-b79e-ac3e303a60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits[0].page_content))  # Print the character count of the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46183c0c-91ec-4609-b7c3-e85c08d8d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034ce612-96d0-4b61-8d53-884b48a85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96fb11-41cb-4d79-b7ec-cc294187baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d04622f-7e3c-413c-89cd-5eef87949966",
   "metadata": {},
   "source": [
    "### Step 3: Store Embeddings\n",
    "\n",
    "- **Description**: Embed the split document content into a vector space and store it in a vector database for later retrieval.\n",
    "\n",
    "- **Key Code Abstractions**:\n",
    "  - Class: `Chroma`\n",
    "  - Method: `from_documents()`\n",
    "  - Class: `OpenAIEmbeddings`\n",
    "\n",
    "- **Code Explanation**:\n",
    "  - **Storing Embeddings**: Use the `Chroma.from_documents()` method to embed all split document chunks using the `OpenAIEmbeddings` model. The resulting vectors are stored in a vector database for fast similarity-based retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f1eca-7d0c-4bcc-b255-054cc002d7f4",
   "metadata": {},
   "source": [
    "#### Chroma Introduction\n",
    "\n",
    "**Initialize a Chroma vector database (instantiation only, without storing vector data):**\n",
    "\n",
    "**1. Initialize via Constructor**: This approach sets up a local persistent Chroma vector store.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "**2. Initialize via Client**: Useful for direct access to the underlying database or collection.\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Use the Chroma.from_documents() method directly for instantiation and data storage:\n",
    "This method returns a Chroma instance of type langchain_chroma.vectorstores.Chroma.\n",
    "Detailed API documentation: https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef288a05-25a4-4f2f-827a-a5f318ff0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Chroma vector store and the OpenAIEmbeddings model to embed and store the split document chunks.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa3545-b456-473a-b620-5fb0eab15dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorstore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b52313-646a-4512-8125-68054c1c7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c587ce-b40f-47ad-bd14-480c49a073e7",
   "metadata": {},
   "source": [
    "### Step 4: Retrieve Documents\n",
    "\n",
    "- **Description**: Use the `VectorStoreRetriever` class‚Äôs `as_retriever()` and `invoke()` methods to retrieve the most relevant document chunks from the vector database based on a query.\n",
    "\n",
    "- **Key Code Abstractions**:\n",
    "  - Class: `VectorStoreRetriever`\n",
    "  - Methods: `as_retriever()`, `invoke()`\n",
    "\n",
    "- **Code Explanation**:\n",
    "  - **Document Retrieval**: Convert the vector store into a retriever and perform similarity search based on the query to get relevant document chunks.\n",
    "  - **Check Retrieval Count**: Print the number of retrieved document chunks to ensure the retrieval was successful.\n",
    "  - **Validate Retrieved Content**: Output the content of the first retrieved document to verify that the results match expectations.\n",
    "\n",
    "In LangChain, all vector databases support the **`vectorstore.as_retriever()`** method to instantiate a corresponding retriever. The returned object is of type `VectorStoreRetriever`.  \n",
    "üìö [API Documentation](https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cdab30e-b07d-4273-b734-3aabff356bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6feddb48-d07e-40be-a9fe-790c3bb75d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.vectorstores.base.VectorStoreRetriever"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9283b2e6-874f-4815-915b-e87da0429a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d292f13-9581-4f28-b54d-ba0fe9fa0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Inspect the content of the retrieved documents\n",
    "print(len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458d6a7d-21a9-49a4-b523-651e3bab51e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc6302-1b59-4abc-813a-6b0d44cc7cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3309c1e-c650-44ae-baba-6f7528372931",
   "metadata": {},
   "source": [
    "### Step 5: Generate Answer\n",
    "\n",
    "- **Description**: Combine the previously built components (retriever, prompt, LLM, etc.) into a complete pipeline that retrieves relevant documents and generates an answer based on the user's question.  \n",
    "  The full chain works as follows: input the user question ‚Üí retrieve relevant documents ‚Üí build the prompt ‚Üí pass it to the model (using the `invoke()` method of the `ChatOpenAI` class) ‚Üí parse the output to produce the final answer.\n",
    "\n",
    "- **Key Code Abstractions**:\n",
    "  - Class: `ChatOpenAI`\n",
    "  - Method: `invoke()`\n",
    "  - Class: `RunnablePassthrough`\n",
    "  - Class: `StrOutputParser`\n",
    "  - Module: `hub`\n",
    "\n",
    "\n",
    "![retrieval](../../images/retrieval.png)\n",
    "\n",
    "\n",
    "#### LangChain Hub\n",
    "\n",
    "[LangChain Hub](https://smith.langchain.com/hub) is an open-source prompt template community that provides developers with ready-to-use prompts. It is part of the **LangSmith** product suite.\n",
    "\n",
    "Below is the prompt template used for the RAG application:  \n",
    "üîó https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "```text\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3dbe88b-7a8f-4607-8426-698a3adf3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488e8c40-a304-4538-a84e-cc8e4e7e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf0de4c1-e389-430b-a472-521960e12b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281d2ea1-0a61-45aa-810d-6b80a7d102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"color yellow\", \"question\": \"What is yellow?\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e55e098-5f2d-4ef6-98e2-b1a4f27fa853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is yellow? \n",
      "Context: color yellow \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Check the prompt\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cbbc6-aaaa-4f8c-8382-1cbda08d2fe9",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è**Using LCEL in RAG**‚≠êÔ∏è\n",
    "\n",
    "##### **Overview of LCEL**\n",
    "\n",
    "LCEL (LangChain Expression Language) is a key concept in LangChain. It provides a unified interface that allows different components (such as `retriever`, `prompt`, `llm`, etc.) to be connected via a shared `Runnable` interface. Each `Runnable` component implements standard methods like `.invoke()`, `.stream()`, and `.batch()`, allowing them to be easily chained using the `|` (pipe) operator.\n",
    "\n",
    "\n",
    "##### **Components Used in LCEL**\n",
    "\n",
    "- **Retriever**: Responsible for retrieving relevant documents based on the user query.\n",
    "- **Prompt**: Builds the prompt using retrieved documents, which is then fed to the LLM.\n",
    "- **LLM**: Accepts the prompt and generates the final answer.\n",
    "- **StrOutputParser**: Parses the LLM output to extract a clean string for display.\n",
    "\n",
    "\n",
    "##### **How LCEL Works**\n",
    "\n",
    "- **Building the Chain**: Using the `|` operator, you can link multiple `Runnable` components into a `RunnableSequence`. LangChain automatically converts some objects into `Runnable`s, such as turning `format_docs` into a `RunnableLambda`, and a dictionary with `\"context\"` and `\"question\"` keys into a `RunnableParallel`.\n",
    "\n",
    "- **Data Flow**: The user‚Äôs question flows through each `Runnable` in the `RunnableSequence`. First, the `retriever` fetches relevant documents. Then, `format_docs` converts the documents into strings. `RunnablePassthrough` passes the original question unchanged. These are passed into the `prompt` to construct a full prompt for the LLM.\n",
    "\n",
    "\n",
    "\n",
    "##### **Key LCEL Operations**\n",
    "\n",
    "- **Format Documents**:  \n",
    "  `retriever | format_docs` passes the question to the retriever and formats the returned documents into a string.\n",
    "\n",
    "- **Pass Through Question**:  \n",
    "  `RunnablePassthrough()` forwards the original question as-is.\n",
    "\n",
    "- **Build Prompt**:  \n",
    "  `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt`  \n",
    "  builds a complete prompt using both the context and the original question.\n",
    "\n",
    "- **Run Model**:  \n",
    "  `prompt | llm | StrOutputParser()` sends the prompt through the model and parses the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe460ec-547e-4714-a7dd-bf9dc319b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66e6049b-af4d-435d-aded-68d9c80803bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct RAG Chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2880d17-79ac-4e5c-966b-144d9ee0dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complicated task into smaller, manageable steps. This can be accomplished using techniques like Chain of Thought (CoT) prompting, which encourages sequential reasoning, or by employing the Tree of Thoughts approach, which explores multiple reasoning paths. It can involve simple prompting, task-specific instructions, or human input to facilitate this breakdown."
     ]
    }
   ],
   "source": [
    "# Stream the answer\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb6b9b0e-9883-4375-8bc1-ea31f3659172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToT, or Tree of Thoughts, is an extension of the Chain of Thought (CoT) prompting technique that involves decomposing a problem into multiple thought steps and generating several thoughts for each step, creating a tree structure. This method allows for exploring various reasoning possibilities using search techniques like BFS or DFS. It enables the model to evaluate each state via a classifier or majority vote for enhanced reasoning capability."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is ToT?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a072d01-60b2-46c7-a104-c8a4a3eda0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d91a2d-56e0-4f52-b6a3-dc893583b51f",
   "metadata": {},
   "source": [
    "### Using self-defined prompt instead of prompt template form hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c5d89d-d642-41cb-ab6f-6ead404b6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always prepeat the question at the beginning of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985266bc-1239-45b1-ab76-d7bd2f279cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(custom_rag_prompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fbe82ee-de48-4cc8-875c-d74242ef273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b818824-5a44-461a-b6d5-3ff079beaaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is Task Decomposition?\\n\\nTask Decomposition is the process of breaking down a complex task into smaller, more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in organizing these steps and exploring multiple reasoning possibilities. This allows models to approach difficult tasks systematically and effectively.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e091c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d9da154",
   "metadata": {},
   "source": [
    "### Using Another Online Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6b1fa5d-674d-4e1c-a423-303aea90415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of page content:  7869\n",
      "first 100 character:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "Please select\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Text-to-SQL Solutions: Top Challenges and Tips to\n"
     ]
    }
   ],
   "source": [
    "# Using a different document \n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"blog--post\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.k2view.com/blog/llm-text-to-sql/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "print(\"length of page content: \", len(docs[0].page_content))\n",
    "print(\"first 100 character: \", docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "895be9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks:  13\n",
      "number of characters in the first chunk 422\n",
      "first chunck content:  Table of Contents\n",
      "\n",
      "\n",
      "Please select\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Text-to-SQL Solutions: Top Challenges and Tips to Overcoming Them\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Text-to-SQL Solutions: Top Challenges and Tips to Overcoming Them7:29\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iris Zarecki\n",
      "Product Marketing Director\n",
      "\n",
      "March 30, 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM-based text-to-SQL is the process of using Large Language Models (LLMs) to automatically convert natural language questions into SQL database queries.\n",
      "first chuck meta data:  {'source': 'https://www.k2view.com/blog/llm-text-to-sql/', 'start_index': 6}\n"
     ]
    }
   ],
   "source": [
    "# Use RecursiveCharacterTextSplitter to split documents into chunks of 1000 characters with a 200-character overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Max size per chunk in characters\n",
    "    chunk_overlap=200, # Overlap between chunks to preserve context across boundaries\n",
    "    add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"number of chunks: \", len(all_splits))\n",
    "print(\"number of characters in the first chunk\", len(all_splits[0].page_content))\n",
    "print(\"first chunck content: \", all_splits[0].page_content)\n",
    "print(\"first chuck meta data: \", all_splits[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71299e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Realizing the potential of LLM text-to-SQL ¬†\n",
      "Using LLMs to generate SQL creates the potential for democratizing data access, enhancing the customer experience, and boosting productivity. However, it also introduces critical challenges in accuracy, performance, and security. ¬†\n",
      "To harness the benefits of LLM-based text-to-SQL, focus on improving schema awareness, using chain-of-thought prompting, establishing robust security measures, and using LLM agents¬†and functions to make it all happen. By addressing these key areas, you can leverage AI-generated SQL to use your data more efficiently, improve your decision-making processes, and safeguard your customers√¢‚Ç¨‚Ñ¢ PII and other sensitive data.\n"
     ]
    }
   ],
   "source": [
    "# Use Chroma vector store and the OpenAIEmbeddings model to embed and store the split document chunks.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"How to generate SQL using LLM?\")\n",
    "# Inspect the content of the retrieved documents\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c602531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate SQL using LLMs, ensure the model is schema-aware by incorporating trusted data sources and metadata for accurate query generation. Employ chain-of-thought prompting to break down queries into simpler steps, enhancing the quality of the output. It's also crucial to implement robust security measures to protect sensitive data throughout the process."
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Construct RAG Chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream the answer\n",
    "# 1 \n",
    "for chunk in rag_chain.stream(\"How to generate SQL using LLM?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ea49e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The limitations of using LLMs to generate SQL include challenges with schema awareness, necessitating rich metadata to ensure accuracy, especially in complex databases. Additionally, the accuracy of generated queries can suffer from issues like AI hallucinations and misunderstood schemas. Performance concerns also arise due to the ambiguity in column names and potential inefficiencies in non-optimized queries."
     ]
    }
   ],
   "source": [
    "# 2\n",
    "for chunk in rag_chain.stream(\"What is the limitation of using LLM to generate SQL?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5512f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To increase accuracy in SQL generated by AI, ensure that the language model (LLM) is schema-aware and can utilize curated data sources. Additionally, implement chain-of-thought prompting to break down queries into simpler steps, which improves the quality of the generated SQL. Finally, focus on addressing ambiguities in column names and complex schemas to minimize misinterpretation."
     ]
    }
   ],
   "source": [
    "# 3\n",
    "for chunk in rag_chain.stream(\"How to increase accuracy in sql generated?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
